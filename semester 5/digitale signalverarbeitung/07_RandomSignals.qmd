\pagebreak

# Random Signals

::: callout-note
## Definition

Vector of a random variable where the *mean* is $m_x=E\{x[n]\}$. Wobei die Autokorrelation eine Funktion Ã¼ber den Abstand zweier samples darstellt

$$
\gamma_{xx}[m]=E\{x^*[n]\cdot x[n+m]\}
$$

![](images/paste-85.png){width="70%"}
:::

## Spectrum

So the mean and autocorrelation characterizes a random signal in the time domain. In the frequency domain a random signal is defined by its **power density spectra**

$$
\Gamma_{xx}(\Omega)=\sum_{m=-\infty}^\infty \gamma_{xx}[m]\cdot e^{-j\Omega m}
$$

::: callout-important
## reverse correlation

One can see, that a wide Autocorrelation corresponds to a small power density spectra

![](images/paste-86.png){width="50%"}
:::

There are several properties derived from *Wiener-Khinchin theorem*

-   The Power $P_x$ of a signal $x[n]$ is

$$
P_x=\frac{1}{2\pi}\int_{-\pi}^\pi\Gamma_{xx}(\Omega){\rm d}\Omega\qquad\text{or}\qquad P_x=\gamma_{xx}[0]
$$

-   The value $P_x=\gamma_{xx}[0]$ is the maximum of the autocorrelation sequence magnitude and

$$
\left|\gamma_{xx}[m]\right|\le P_x\hspace{4mm}\mbox{for all }m
$$

-   For stationary random signals with nonzero mean $m_x$, $\gamma_{xx}[m]=c_{xx}[m]\cdot e{-j\Omega m}+m_x^2\cdot 2\pi\delta(\Omega)$ holds

$$
\Gamma_{xx}(\Omega)=\sum_{m=-\infty}^\infty c_{xx}[m]\cdot e^{-j\Omega m}+m_x^2\cdot 2\pi\delta(\Omega)
$$

::: callout-caution
## White noise

A special class of stationary stochastic processes are *white noise* signals. These are characterized by an autocorrelation of the form

$$
\gamma_{ww}[m]=\left\{\begin{array}{ll}\sigma^2_w & \mbox{if }m=0 \\ 0 & \mbox{if }m\ne0\end{array}\right.
$$

implying that consecutive samples are uncorrelated. The corresponding power density spectrum is constant over $\Omega$, given as

$$
\Gamma_{ww}(\Omega)=\sigma^2_w
$$
:::

## Spectral Shaping in LTI Systems

If we pass a random signal through a LTI-System

![](images/dsp202img1.png){width="40%"}

we get the following output signal characteristics

> **mean:** $m_y=H(0)\cdot m_x$
>
> **autocorrelation**: $\gamma_{yy}[m]=h^*[-i]*\gamma_{xx}[m]*h[k]$
>
> **spectra**: $\Gamma_{yy}(\Omega)=|H(\Omega)|^2\cdot\Gamma_{xx}(\Omega)$
>
> **z-transformation**: $\Gamma_{yy}(z)=H(z^{-1})\cdot\Gamma_{xx}(z)\cdot H(z)$

## Linear Models for Stochastic Processes

First we apply **filtering of white noise** through the system $H(z)$

::: {layout="[40,60]" layout-valign="center"}
![](images/dsp203img1.png){width="40%"}

$$
\Gamma_{yy}(z)=\sigma_w^2\cdot H(z)\cdot H(z^{-1})
$$
:::

Making up a filter by picking zeros and poles lying within the unit circle for guaranteed stability of the system we get the filter $H(z)$ and the corresponding power density spectra $\Gamma_{yy}(z)$

$$
H(z)=\frac{B(z)}{A(z)}=\frac{\sum\limits_{k=0}^M b_kz^{-k}}{1+\sum\limits_{k=1}^N a_kz^{-k}}
\qquad\text{thus}\qquad
\Gamma_{yy}(z)=\sigma_w^2\cdot\frac{B(z)\cdot B(z^{-1})}{A(z)\cdot A(z^{-1})}
$$

By taking the inverse $\frac{1}{H(z)}$ of the system we get a **noise whitening filter** with the properties

::: {layout="[40,60]" layout-valign="center"}
![](images/dsp203img3.png){width="40%"}

$$
\frac{1}{H(z)}=\frac{A(z)}{B(z)}=\frac{1+\sum\limits_{k=1}^N a_kz^{-k}}{\sum\limits_{k=0}^M b_kz^{-k}}
$$
:::

In the following we distinguish three different models for white noise filters

| Model               | $H(z)$ | $\frac{1}{H(z)}$ |
|---------------------|:------:|:----------------:|
| Moving Average *AV* |  FIR   |       IIR        |
| Autoregressive *AR* |  IIR   |       FIR        |
| ARMA                |  IIR   |       IIR        |

## Moving average *(MA)* model

If $H(z)$ represents an FIR filter of order $M$, the white noise $w[n]$ is transformed into the random signal

$$
y[n]=\sum_{k=0}^M b_kw[n-k]
$$

In the special case of $b_0=b_1=\cdots=b_M=(M+1)^{-1}$, every sample at the filter output represents the average of the input signal within a sliding window of length $M+1$

![](images/paste-87.png){width="40%"}

For finite $M$ the pole-zero plot of $H(z)$ contains only zeros. By inversion $\frac{1}{H(z)}$ every zero gets substituted by a pole, thus making it a all-pole filter.

The following holds for the autocorrelation

$$
\gamma_{yy}[m]=
\begin{cases}
\sigma_w^2\sum_{k=m}^M b_k\cdot b_{k-m}^* & \mbox{if }0\le m\le M \\ 
0 & \mbox{if }m>M \\ 
\gamma_{yy}^*[-m] & \mbox{if }m<0
\end{cases}
$$

**Example**: $H(z)=0.6+0.8z^{-1}$

![](images/dsp203img5.png){width="70%"}

## Autoregressive *(AR)* model

In the so-called *autoregressive* (AR) model, a random sequence is generated by an all-pole filter according to

$$
y[n]=w[n]-\sum_{k=1}^N a_ky[n-k]\qquad\laplace\qquad H(z)=\frac{z^N}{z^N+a_1z^{N-1}+\dots+a_N}
$$

The corresponding noise whitening filter is a causal FIR filter with no poles in the pole-zero plot. To determine the filter coefficients $a_1,a_2,\dots,a_N$ we need the noise variance $\sigma_w^2$ and the following equations

$$
\gamma_{yy}[m]=
\begin{cases}-\sum_{k=1}^N a_k\cdot \gamma_{yy}[m-k] & \mbox{if }m>0 \\ \sigma_w^2-\sum_{k=1}^N a_k\cdot \gamma_{yy}[-k] & \mbox{if }m=0 \\ \gamma_{yy}^*[-m] & \mbox{if }m<0
\end{cases}
$$

$$
\left(\begin{array}{cccc}\gamma_{yy}[0] & \gamma_{yy}[-1] & \cdots & \gamma_{yy}[-N] \\ \gamma_{yy}[1] & \gamma_{yy}[0] & \cdots & \gamma_{yy}[-N\!+\!1] \\ \vdots & \vdots & & \vdots \\ \gamma_{yy}[N] & \gamma_{yy}[N\!-\!1] & \cdots & \gamma_{yy}[0] \end{array}\right)\cdot\left(\begin{array}{c}1 \\ a_1 \\ \vdots \\ a_N \end{array}\right)=\left(\begin{array}{c}\sigma_w^2 \\ 0 \\ \vdots \\ 0 \end{array}\right)
$$

The desired coefficients can be found by solving the *Yule-Walker equations*.

## ARMA model

The MA and AR models are special cases of the general so-called ARMA model. Here the orders of the polynomials $B(z)$ and $A(z)$ (i.e., $M$ and $N$, respectively) are both one or higher. Hence the pole-zero plots of both $H(z)$ and $\frac{1}{H(z)}$ contain poles *and*
zeros. For some random processes the ARMA model is more suitable than the above discussed special versions because of a smaller number of required coefficients $b_1,b_1,\dots,b_M,a_1,a_2,\dots,a_N$ for an accurate modeling.

# Spectral Density Estimation

## Nonparametric methods

Directly computing the DTFT squares leads to the **periodogram**

$$
\widehat\Gamma_{xx}(\Omega)=\frac1N\left|\sum_{n=0}^{N-1}x[n]\cdot e^{-j\Omega n}\right|^2
$$

Because for any $\Omega_0$ the estimate $\widehat{\Gamma}{xx}(\Omega_0)$ has a large variance, which doesn't decrease with increasing sample basis we introduce the following

-   Biased autocorrelation estimator:

$$
\widehat\gamma_{xx}[m]=\frac1N\sum_{n=0}^{N-m-1}x^*[n]\cdot x[n+m],\hspace{4mm}\mbox{for }m\mbox{ between 0 and }N-1
$$

-   Unbiased autocorrelation estimator:

$$
\widehat\gamma_{xx}[m]=\frac1{N-m}\sum_{n=0}^{N-m-1}x^*[n]\cdot x[n+m],\hspace{4mm}\mbox{for }m\mbox{ between 0 and }N-1
$$

## Parametric methods

Parametric methods build on a specific model tuned by a fixed number of parameters. Usually a ARMA model is used to get a good system property. But also AR and MA models can be of good use. In a AR model the parameters can be deduced from an estimate of the autocorrelation sequence by means of the Yule-Walker equations. Substituting $\gamma_{xx}[m]$ with $\widehat{\gamma}_{xx}[m]$.

Another way to compute the parameters $a_1,\dots,a_k$ is through *least squares* model fitting. According to the AR model the $n$th sample can be written as $y[n]=\hat{h}[n]+w[n]$ with

$$
\hat y[n]=-\sum_{k=1}^N a_ky[n-k]
$$