---
format:
  chribel-summary-quarto-pdf:
    include-in-header:
    - text: "\\usepackage[datesep=.]{datetime2}"
    - text: "\\DTMsetdatestyle{ddmmyyyy}"
    - text: "\\usepackage{blindtext}"
    toc: true
    classoption: twocolumn

# [DOCUMENT INFORMATION]
title: "Deep Learning"
subtitle: "COSC440"
author: "Andy Ming"

# [PAGE OPTIONS]
lang: enGB
babel-lang: ukenglish

# [HEADER & FOOTER]
fancyhdr:
  header:
    right: "Deep Learning"
    center: ""
    left: "University of Canterbury"
  footer:
    right: "COSC440"
    center: "\\thepage\\ / \\pageref{LastPage}"
    left: "\\today"
  
source:
  github: "https://www.youtube.com/watch?v=VGhcSupkNs8"

accentcolor: "124E82" # must be given as hex, sadly :(

chribel-fontfamily:
  - name: AlegreyaSans      # used for section headings, title page
  - name: cmbright          # used for paragraph and math
  - name: inconsolata
    options: "scaled=0.95"  # for code blocks
---

# Details

## Science of Arrays

Don't loop over elements in a array. Use numpy functions to do elementwise operations:

``` python
# Elementwise sum; both produce an array
z = x + y
z = np.add(x, y)
```

Use **Boradcasting** to work with arrays of different sizes. In Hardware data is take from the same memory space multiple times.

``` python
# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 2])
y = x + v.T  # Add v to each row of x using broadcasting
print(y)  # Prints "[[ 2  2  4]
          #          [ 5  5  7]
          #          [ 8  8 10]
          #          [11 11 13]]"
```

Do **Matrix Multiplications**, remember that matrices of shape $100x20 \times 20x40$ equal a output shape of $100x40$:

``` python
C = np.dot(A,B)
F = np.matmul(D,E)
```

The **Reason** is that this code is optimised for fast computation. Manly due to the utilisation of GPUs which offer high parallelism. *Don't bother trying to implement a faster version*.

![](images/paste-19.png){fig-align="center" width="9cm" height="1.7cm"}

![](images/paste-20.png){fig-align="center" width="7cm" height="4.6cm"}

![](images/paste-21.png)

## Morals of AI

### Gender Shades

Gender detecting algorithm is trained on a highly biased dataset, which leads to different results, depending on the target. [Read More](http://gendershades.org/)

![](images/paste-39.png){fig-align="center" width="8cm" height="2.7cm"}

::: callout-caution
## Beyond Average Test-Set Performance

Even if a test-set is a well representation of the real world, which will lead to good average accuracy. The network can still do badly on minority group test sets.

**Good average performance can mask poor performance of specific cases**

![](images/paste-45.png){fig-align="center" width="8cm" height="3cm"}
:::

### Reknognition

Facial Recognition algorithm wrongly matches government members to mugshots from a criminal database. The algorithm had **5% false positives** which isn't to bad, but when deployed states big issues of wrongly accusing innocent people. [Read More](https://www.aclu.org/news/privacy-technology/amazons-face-recognition-falsely-matched-28)

# Machine Learning Concepts

**Machine Learning == Function Approximation**

![](images/paste-1.png){fig-align="center" width="6cm" height="3.3cm"}

![](images/paste-4.png){fig-align="center" width="7cm"}

## Types of Learning

![](images/paste-2.png)

![](images/paste-3.png){fig-align="center" width="9cm" height="3.5cm"}

### Self-Supervised Learning

From data **without labels** we can learn the structure of the data itself, there are several approaches, the basic idea is dimensionality reduction:

-   K-Means Clustering
-   Principal Component Analysis (PCA)
-   Butterfly-Network (Autoencoder)
-   ...

## Types of Problems

## Maschine Learning Pipeline

![](images/paste-6.png)

### Dataset

*Annotated Datasets* like [MNIST](https://yann.lecun.com/exdb/mnist/) (Handwritten digits).

### Preprocessing

Split the dataset into **Train, Validation, and Test sets**

![](images/paste-7.png){fig-align="center" width="8cm"}

### Train Model

1.  **Initialization**: Set all weights $w_i$ to 0.
2.  **Iteration Process**:
    -   Repeat for $N$ iterations, or until the weights no longer change:
        -   For each training example $\mathbf{x}^k$ with label $a^k$:
            1.  Calculate the prediction error:
                -   If $a^k - f(\mathbf{x}^k) = 0$, continue (no change to weights).
            2.  Otherwise, update each weight $w_i$ using: $$ w_i = w_i + \lambda \left( a^k - f(\mathbf{x}^k) \right) x_i^k $$
    -   where $\lambda$ is a value between 0 and 1, representing the learning rate.

## Optimizing with Gradient Descent

### Loss Function

Function $L$ which measures how "wrong" a network is. We want our network to answer right with **high probability**.

![](images/paste-11.png){fig-align="center" width="9cm" height="2.9cm"}

To get a probability for **binary classification**, we introduce a **probability layer**. One of the possible function is **Softmax**

$$
p_j=\frac{e^{l_j}}{\sum_k e^{l_k}}
$$

For every output $j$ it takes every logit (output of network before activation/probability is applied) $l_j$ in the exponent to ensure positivity. Dividing it by the sum of all logits ensures that $\sum_kp_k=1$.

To get the loss $L$ we apply a loss-function, *low probability* $\rightarrow$ *high loss*. We use **Cross Entropy Loss**

![](images/paste-13.png)

### Gradient Descent

$\Delta w_{j,i}=-\alpha\frac{\partial L}{\partial w_{j,i}}$

| $\alpha$: learning rate *(typically 0.1-0.001)*
| $L$: loss function
| $w_{j,i}$: one single weight

To compute $-\alpha\frac{\partial L}{\partial w_{j,i}}$ use the chain rule

![](images/paste-12.png)

``` python
## Backpropagation on batch learning
# y = expected - (f(x)>0)
labels_OH = np.zeros((labels.size, self.num_classes), dtype=int)
labels_OH[np.arange(labels.size),labels] = 1  # One-Hot encoding
predictions = np.argmax(outputs, axis=1)
predictions_OH = np.zeros_like(outputs)
predictions_OH[np.arange(outputs.shape[0]), predictions] = 1
y = labels_OH - predictions_OH
# db = y*1
gradB = np.mean(y, axis=0)   # average over batch
# dW = y*x
y = y.reshape((outputs.shape[0],1,self.num_classes))
inputs = inputs.reshape((outputs.shape[0],self.input_size[0]*self.input_size[1],1))
dW = inputs*y
gradW = np.mean(dW, axis=0)  # average over batch
```

### Stochastic Gradient Descent (SGD)

Train a network on **batches**, small subsets of training data.

``` python
# Stochastic Gradient Descent
for start in range(0, len(train_inputs), model.batch_size):
    inputs = train_inputs[start:start+model.batch_size]
    labels = train_labels[start:start+model.batch_size]
    # For every batch, compute then descend the gradients for the model's weights
    outputs = model.call(inputs)
    gradientsW, gradientsB = model.back_propagation(inputs, outputs, labels)
    model.gradient_descent(gradientsW, gradientsB)
```

-   Training process is *stochastic* / *non-deterministic*: batches are a random subsample.
-   The gradient of a random-sampled batch is a unbiased estimator of the overall gradient of the dataset.
-   Pick a large enough batch size for s*table updates*, but small enough to *fit your GPU*

### Optimization

## Automatic Differentiation

To avoid having to recalculate the whole chain every time a new layer is added, we use *automatic derivation*. There are several options:

### Numeric differentiation

-   $\frac{df}{dx}\approx\frac{f(x+\Delta x)-f(x)}{\Delta x}$
-   Called *finite differences*
-   Easy to implement
-   Arbitraritly inaccurate/unstable

### Symbolic differentiation

-   $\frac{dx^2}{dx}=2x$
-   Computer does algebra and simplifies expressions
-   Very exact
-   Complex to implement
-   Only handles static expressions

### Automatic differentiation

-   Use the chain rule at runtime
-   Gives exact results
-   Handles dynamics
-   Easier to implement
-   Can't simplify expressions

#### Forward Mode Autodiff

Every node stores its `(value, derivative)` in a tuple, called **dual numbers**. To compute the overall derivative, each derivative can be chained up. This is implemented via **Overloading**, every function / operator has multiple definitions based on the types of the arguments. ML-Framwork functions work on these tuples.

![](images/paste-14.png)

**Time Effect**: $O(N*M)$ time, $O(1)$ memory, with $N$ = number of inputs, with $M$ = number of nodes

::: callout-note
## Issue w/ forward mode

![](images/paste-15.png)
:::

#### Reverse Mode Autodiff

First, run the function to produce the graph, then compute the **derivatives backward**.

![](images/paste-16.png){fig-align="center" width="7cm"}

-   Analog to the forward mode: overload math functions/operators
-   Overloaded function return *Node* objects
-   Overloaded functions build compute graph while executing
-   After forward pass, the operations are recorded
-   The backwards pass walks along the graph and computes the derivatives
-   **Time Effect**: $O(M)$ time, $O(M)$ memory, with $M$ = number of nodes

#### Fan-Outs (Reverse)

The way to handle fan-out is to **add** the derivatives of the fanned-out nodes through replication $r(x)$.

![](images/paste-17.png)

## Diagnosis Problems

### Overfitting

Training on a complex dataset can lead to **overfitting** ([PTB](https://paperswithcode.com/dataset/penn-treebank)is a language dataset).

![](images/paste-40.png){fig-align="center" width="8cm"}

### Regularization

This approach **modifies the loss** through adding a additional term to our existing loss function.

![](images/paste-44.png)

Regularization can be applied to certain layers on Keras through `tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(lambda), activation='relu')`.

# Deep Learning Concepts

::: callout-caution
## Common Misconception

**Deep Learning != AI**, Just because deep learning algorithms are used doesn't mean there is any intelligence involved.

**Deep Learning != Brain**, Modern deep nets don't depend solely on *biologically mimiced neural nets* any more. A fully connected layer represents such a neural net the closest.

**Deep Learning ==**:

1.  *Differentiable functions*, composed to more complex diff. func.
2.  A deep net is a differentiable function, some inputs are *optimizable parameters*
3.  Differentiable functions produce a computiation graph, which can be traversed backwards for *gradient-based optimization*
:::

## Multi-Dimensional Arrays & Memory Models

### Vectorized Operations

For efficient operation, use **Matrices**.

![](images/paste-18.png)

## Neural Networks

### Perceptron

![](images/paste-8.png){fig-align="center" width="8cm"}

**Predicting with a Perceptron**:

1.  Multiply the inputs $x_i$ by their corresponding weight $w_i$
2.  Add the bias $b$
3.  **Binary Classifier**, greater than 0, return 1, else return 0

$$
f_{\Phi}(\mathbf{x}) =\begin{cases} 1, & \text{if } b + \mathbf{w} \cdot \mathbf{x} > 0 \\0, & \text{otherwise}\end{cases}
$$

::: callout-important
## Parameters

**Weights**: "importance of the input to the output"

-   Weight near 0: Input has little meaning to the output
-   Negative weight: Increasing input $\rightarrow$ decreasing output

**Bias**: "a priori likelihood of positive class"

-   Ensures that even if all inputs are 0, there is some result
-   Can also be written as a weight for a constant $1$ input

\begin{align*}
[ x_0, x_1, x_2, \dots, x_n ] \cdot [ w_0, w_1, w_2, \dots, w_n ] + b \\
= [ x_0, x_1, x_2, \dots, x_n, 1 ] \cdot [ w_0, w_1, w_2, \dots, w_n, b ]
\end{align*}
:::

**Multi-Class Perceptron**

![](images/paste-9.png){fig-align="center" width="8cm"}

**Biary Classifier**: Only one output can be active $\hat{y} = \text{argmax}\left(f\left(x^k\right)\right)$, thus the update terms are

$$
\Delta w_i=
\begin{cases}
0, &\text{for }a^k=\hat{y} \\
-x_i^k, &\text{for }\hat{y}=1,\space a^k=0 \\
x_i^k, &\text{for }\hat{y}=0, \space a^k=1
\end{cases}
$$

### Multi-Layer

Through adding hidden layers we can make bigger networks and add more states to the algorithm.

![](images/paste-22.png)

The size of these **hidden layers** are defined by the **hyperparameter**. These define the configuration of a model and are set before training begins. *Rule of Thumb*: Make hidden layers the same size as the input, then start to tweak to see the effect. If you have more time and money, [check this](https://en.wikipedia.org/wiki/Hyperparameter_optimization).

![](images/paste-27.png){fig-align="center" width="7cm"}

::: callout-note
## Universal Approximation Theorem

Remarkably, a one-hidden-layer network can actually represent any function (under the following assumptions):

-   Function is continuous
-   We are modeling the function over a closed, bounded subset of $\mathbb{R}^n$
-   Activation function is sigmoidal (i.e. bounded and monotonic)

![](images/paste-28.png)
:::

::: callout-warning
## Stacking Linear Layers isn't Enough

When simplifying the linear equation we get

$$
\sigma\left([\begin{smallmatrix} w_2 & b_2 \end{smallmatrix}]
\left([\begin{smallmatrix} w_1 & b_1 \end{smallmatrix}]\left[\begin{smallmatrix} x\\1 \end{smallmatrix}\right]\right)\right) =
\sigma\left([\begin{smallmatrix} w_{12} & b_{12} \end{smallmatrix}]\left[\begin{smallmatrix} x\\1 \end{smallmatrix}\right]\right)
$$

Which is exactly the same as just one layer again, we need **activation**.
:::

### Activation Functions

We introduce a **nonlinear** layer

![](images/paste-10.png){fig-align="center" width="7cm"}

A activation function binds network outputs to a particular range. In the last layer this can be used to restrict the range, for example *age is strictly positive*.

Futher PyTorch activation functions can be found [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).

#### Sigmoid

.

![](images/paste-23.png)

#### Tanh

.

![](images/paste-24.png)

::: callout-warning
## Vanishing Gradient

The problem with **Sigmoid** and **Tanh** is that the further away the parameters get from zero, the smaller is the gradient. Thus the network stops learning at these points. When **stacking layers** the issue gets even more severe.
:::

#### ReLU

.

![](images/paste-25.png)

::: callout-warning
## Dead ReLU

Because the negative part fed into the activation will result in a $0$ output. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again.
:::

#### Leaky ReLU

To tackle a possible *dead ReLU* issue, we use a tiny positive slope for negative inputs.

![](images/paste-26.png)

### Convolution

Convolution is like a *"partially connected"* layer. Only certain inputs are connected to certain output pixels.

To introduce **translational invariance** $f(T(x))=f(x)$ we apply convolutions. These are "Filters" which highlight different structures, the following network makes sense from the structures, not the pixels itself. Main application: **Computer Vision**.

![](images/paste-29.png){fig-align="center" width="7cm" height="2.2cm"}

::: callout-note
## Hyperparameters

There are 4 hyperparameters for the convolution

-   Number of filters, $n$
-   Size of these filters, $n$
-   The Stride, $s$
-   Amount of padding, $p$

![](images/paste-34.png){fig-align="center" width="8cm"}

We can calculate the output size through

$$
w'=\frac{w-f+2p}{s}+1,\space h'=\frac{h-f+2p}{s}+1, \space d'=n
$$

For **VALID** padding $p=0$, for **SAME** padding $p$ is chosen so output is same
:::

``` python
# Execute manual Convolution
# Should be of shape (batch_sz, 32, 32, 3) for CIFAR10 
inputs = CIFAR_image_batch
# Sets up a 5x5 filter with 3 input channels and 16 output channels 
self.filter = tf.Variable(tf.random.normal([5, 5, 3, 16], stddev=0.1))
# Convolves the input batch with our defined filter 
conv = tf.nn.conv2d(inputs, self.filter, [1, 2, 2, 1], padding="SAME")
```

The inputs to `tf.nn.conv2d(...)` are:

-   `input` = \[batchSz, input_height, input_width, input_channels\]
-   `filter` = \[f_height, f_width, in_channels, out_channels\]
-   `strides` = \[batch_stride, stride_along_height, stride_along_width, stride_along_input_channels\]
-   `padding` = either `'SAME'` or `'VALID'`

Tipically there are several convolutional layers and then a fully connected layer. This can be achieved through flattening a layer `flat = tf.reshape(conv, [conv.shape[0],-1])`.

![](images/paste-36.png)

#### Stride

The distance we slide a filter on each iteration is called **stride**. With a bigger stride, you compress a same size input into a smaller output. This decreases the image resolution controlled, **Downsampling**. The filters are **Kernels** and are made of **learnable parameters**.

![](images/paste-30.png)

##### Computation

The image is flattened and the kernel is unrolled into a bigger matrix. This leads to a normal **matrix/vector multiplication**

![](images/paste-56.png)

#### Fractional Stride

For **deconvolutions** you can also use fractionally-strided convolutions (here $\frac12$ stride):

![](images/paste-55.png){fig-align="center" width="7cm" height="3.9cm"}

##### Computation

Same as with the convolution, we flatten all matrices into vectors/matrices but now we **transpose** the kernel matrix, which gives us the **de-convolution**:

![](images/paste-57.png){fig-align="center" width="5cm"}

::: callout-caution
## Checkerboard Artifacts

THe transpose convolution causes [artifacts in output images](https://distill.pub/2016/deconv-checkerboard/) because some pixels get written more often than others (at the overlaps, which occur in a line).

![](images/paste-58.png){fig-align="center" width="6cm" height="1.5cm"}

*Prevention*:

1.  Upsampling using nearest neighbour interpolation

![](images/paste-59.png){fig-align="center" width="7cm"}

2.  Perform a convolution with `'SAME'` padding on upsampled image

![](images/paste-60.png){fig-align="center" width="8cm"}

``` python
# Layer to upsample the image by a factor of 5 in x and y using nearest 
# neighbor interpolation
tf.keras.layers.UpSampling2D(size=(5, 5), interpolation=’nearest’)
# Do a convolutional layer on the result 
tf.keras.layers.Conv2D(filters = 1, kernel_size = (10,10), padding = “SAME”)
```
:::

#### Filter Banks

Futhermore, use several kernels per image, this block of kernels is a **filter bank**. The output is then a **mutli-channel** image. Multiple filters are able to extract *different features* of the image.

![](images/paste-31.png){fig-align="center" width="8cm"}

#### Padding

To not loose resolution through a convolution, the original image has to be extended, **padded**. There are two convolution options, **VALID**, which is without padding, or **SAME** which is padding so that the output size is same as the input size.

![](images/paste-32.png){fig-align="center" width="8cm" height="4cm"}

#### Bias

As with other layers, a **Bias** can be added to the convolutional layer

![](images/paste-35.png){fig-align="center" width="8cm"}

This can be done through `tf.nn.bias_add(value, bias)`. When using keras layers, a bias is included by default `tf.keras.layers.Conv2D(filters, kernel_sz, strides, padding, use_bias = True)`.

### Pooling

Pooling keeps track of the regions with the highest activation, indicating object presence, also lowers the resolution in a controllable way.

![](images/paste-37.png){fig-align="center" width="8cm"}

### Invariances

The translational variance is largely eliminated with the introduction of convolutions, this can be further improved by introduction of [antialising](https://arxiv.org/pdf/1904.11486).

There are further invariances, which can hurt a CNNs performance. CNNs don't do to good on these, for good performance, lots of training is needed.

![](images/paste-38.png){fig-align="center" width="7cm"}

## Sequential and Recurrent Networks

## Latent Space

**Latent Space** is a compact representation, representing the input in a lower dimension. This is used for **Self-Supervised Learning**.

Represent the data with fewer dimensions, although data might exist in high dimensional space, it actually may exist along a lower dimensional subspace (e.g. 2D-Plane in 3D-Space, Line in 2D-Space), **Dimensionality Reduction.**

We do this for **smaller dataset footprint (memory)**, more efficient search through **nearest neighbour algorithms**, many **clustering algorithms behave better** in lower dimensions, Easier to **visualize**.

### Principal Component Analyis (PCA)

![](images/paste-46.png){fig-align="center" width="9cm"}

Given a dataset $D$ of dimension $n$ and a target dimension $m\leq n$, find $m$ vectors in $\mathbb{R}^n$ along which $D$ has the highest variance. $m$ are the **principal components**.

How: Find the direction of maximal variation, project onto this vector, repeat $m$ times.

*Example with MNIST:*

![](images/paste-47.png){fig-align="center" width="8cm"}

::: callout-caution
## Limitations of PCA

PCA can't figure out **non-linear** projections from $\mathbb{R}^2$ to $\mathbb{R}^1$.

![](images/paste-48.png){fig-align="center" width="7cm" height="2cm"}
:::

### Butterfly-Network (Autoencoder)

Because we don't have labels, we copy the first part of the network and inverse it. The loss is then calculated through

$$
L(x,\bar{x})=(x-\bar{x})^2\qquad\text{squared error loss}
$$

![](images/paste-49.png){fig-align="center" width="9cm"}

After training the network with the unlabelled data, the second part can be cut off and a binary layer be added. Now fine tune the last layer on **labelled data through supervised learning**.

![](images/paste-50.png){fig-align="center" width="7cm" height="2.9cm"}

### Convolutional Autoencoder

The same approach can be applied to convolutional networks. But **convolutional matrices are expensive**, thus we just swap the forward and backwards pass code for the **deconvolution**.

![](images/paste-53.png){fig-align="center" width="8cm" height="1.9cm"}

This can be done in [Tensorflow](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/conv2d_transpose)

![](images/paste-54.png){fig-align="center" width="10cm"}

### Autoencoder Applications

#### Denoising

Networks can be trained to denoise images. The training is easy, as you just add the noise to original images.

![](images/paste-51.png){fig-align="center" width="8cm" height="2.3cm"}

#### Anomaly / Novelty Detection

.

![](images/paste-52.png)

#### Others

-   Transformer
-   Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)
-   Subspace, correlation-based, and tensor-based outlier detection for highdimensional data
-   One-class support vector machines
-   Replicator neural networks, autoencoders, and long short-term memory neural networks
-   Bayesian Networks
-   Hidden Markov models (HMMs)
-   Cluster analysis-based outlier detection
-   Deviations from association rules and frequent itemsets
-   Fuzzy logic-based outlier detection;

## Transfer Learning

## Training Methods and Tricks

### Early Stopping

If one stops training when the testing loss is starting to rise, at least the loss won't get bigger

``` python
# Pseudo Code Early Stopping
curr_test_loss = inf 
for i in range(n_epochs):
    train model()
    new_test_loss = model.get_test_loss() 
    if new_test_loss > curr_test_loss:
        break
    else:
        curr_test_loss = new_test_loss
```

### Reduce Parameters

Reducing parameters, means less possibilities to learn or even memorize a dataset.

![](images/paste-41.png){fig-align="center" width="8cm"}

Reducing parameters can mean...

-   ... reducing layer size
-   ... decrease number of channels in a convolution
-   ... decrease number of layers

::: callout-tip
## Reducing Parameters

Can also be used to check if parts of a network **are actually needed** $\rightarrow$ remove part $\rightarrow$ retrain model $\rightarrow$ if it behaves the same, it wasn't needed.

Requires obnoxious tuning of hyperparameters.
:::

### Data Agumentation

Generate **random variations** on your training data.

![](images/paste-42.png)

### Dropout

*Make it harder for the network*. In a single training pass, the output of randomly selected nodes from each layer are set to $0$. The nodes that drop out are **different each pass**. This builds **Resillience**. During testing all nodes are active again.

![](images/paste-43.png){fig-align="center" width="7cm" height="3.8cm"}

Dropout can be handled with Keras through `tf.keras.layers.dropout(rate)`, where `rate`is a *hyperparameter* between $[0,1]$. `rate=0.5` is drop $\frac12$, keep $\frac12$. `rate=0.25` is drop $\frac14$, keep $\frac34$.

### Skip Connections / Residual Blocks

::: callout-caution
## Vanishing Gradients

The deeper a net gets (more layers) the more learnable parameters are present. This leads to vanishing of the gradient throughout the network

![](images/paste-61.png){fig-align="center" width="5cm" height="3.1cm"}
:::

To mitigate the *vanishing gradient problem*, we use **Redidual Blocks**

![](images/paste-62.png){fig-align="center" width="7cm"}

The output of each layer is the identity + some deviation (residual) from it. It allows the gradient to flow through two pathways. **Significantly stabilises training of very deep networks.**

### Batch Normalisation

The idea is to stabilise inputs, which should lead to **faster training**. This is done through normalisation of the layers' inputs by re-centring and re-scaling.

A **normalisation layer** $BN$ is added after a fully connected or a convolution, before the non-linear activation. In*Tensorflow* this is done with `tf.keras.layers.BatNormalization(input)`.

![](images/paste-63.png)

![](images/paste-64.png){fig-align="left" width="7cm" height="3cm"}

### Checkpointing

::: callout-warning
## Training is Expensive

Training takes a while and sometimes you want to “save” data for use in future instances.

**Checkpointing** allows you to save your Tensorflow model! No need to retrain every time your run your program. Fast prediction. Export your trained weights for use in other applications
:::

![](images/paste-65.png){width="7cm" height="3cm"}

Example: (more [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/checkpoint.ipynb))

![](images/paste-66.png){width="8cm" height="3.6cm"}

### Xavier Initialisation

::: callout-warning
## Bad Initialization

With certain activation functions, it's possible to get bad initialisations. For example with the sigmoid function we have the issue of linearity around $x\approx0$ and the flatness / low gradient at $x>|4|$.

![](images/paste-67.png){fig-align="center" width="6cm"}

One solution is to use activations which don't have these issues, like **ReLU**.

Event then we want to keep values in the same range when they flow through a network. Values that drastically fluctuate in magnitude can lead to **numerical instability** $\rightarrow$ *slow convergence*.

Consider a weight matrix $W$ of size $m × n$ :

-   One entry $y_i$ of the product $Wx$ is: $y_i = W{i,1}x_1 +\cdots + W{i,j}x_j + \cdots+ W_{i,n}x_n$
-   If $n$ increases, then the magnitude of $y_i$ would also tend to increase
-   If $m$ increases, then the output vector $Wx$ would have a larger dimension
-   As the output becomes the input for the next layer, the next layer would add up in terms

We want the magnitude of weights to be **inversely proportional** to $m$ and $n$.
:::

To tackle this issue, we use the **Xavier Initialization**, we calculate the *standard deviation* on each layer $i$ new:

$$
\sigma_i=\sqrt{\frac2{n_i+m_i}}
$$

### Keras

Tensorflow code frequently gets terribly cumbersome, [**Keras**](https://keras.io/) **shortens the amount of code** you need to write through higher-level APIs for constructing, training, and evaluating models.

### Tensorboard (Visualization)

A powerful visualisation, logging, and monitoring tool designed to be integrated with Tensorflow (although you can technically use it with any Python code).

[Check the Demo](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master%20/docs/get_started.ipynb)

# Deep Learning Problems, Models & Research

## Computer Graphics and Vision

## Natural Language

## Audio and Video Synthesis

## Search using Deep Reinforcment Learning

## Anomaly Detection

## Irregular Networks