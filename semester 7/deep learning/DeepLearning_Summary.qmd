---
format:
  chribel-summary-quarto-pdf:
    include-in-header:
    - text: "\\usepackage[datesep=.]{datetime2}"
    - text: "\\DTMsetdatestyle{ddmmyyyy}"
    - text: "\\usepackage{blindtext}"
    toc: true
    classoption: twocolumn

# [DOCUMENT INFORMATION]
title: "Deep Learning"
subtitle: "COSC440"
author: "Andy Ming"

# [PAGE OPTIONS]
lang: enGB
babel-lang: ukenglish

# [HEADER & FOOTER]
fancyhdr:
  header:
    right: "Deep Learning"
    center: ""
    left: "University of Canterbury"
  footer:
    right: "COSC440"
    center: "\\thepage\\ / \\pageref{LastPage}"
    left: "\\today"
  
source:
  github: "https://www.youtube.com/watch?v=VGhcSupkNs8"

accentcolor: "124E82" # must be given as hex, sadly :(

chribel-fontfamily:
  - name: AlegreyaSans      # used for section headings, title page
  - name: cmbright          # used for paragraph and math
  - name: inconsolata
    options: "scaled=0.95"  # for code blocks
---

# Details

## Science of Arrays

Don't loop over elements in a array. Use numpy functions to do elementwise operations:

``` python
# Elementwise sum; both produce an array
z = x + y
z = np.add(x, y)
```

Use **Boradcasting** to work with arrays of different sizes. In Hardware data is take from the same memory space multiple times.

``` python
# We will add the vector v to each row of the matrix x,
# storing the result in the matrix y
x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])
v = np.array([1, 0, 2])
y = x + v.T  # Add v to each row of x using broadcasting
print(y)  # Prints "[[ 2  2  4]
          #          [ 5  5  7]
          #          [ 8  8 10]
          #          [11 11 13]]"
```

Do **Matrix Multiplications**, remember that matrices of shape $100x20 \times 20x40$ equal a output shape of $100x40$:

``` python
C = np.dot(A,B)
F = np.matmul(D,E)
```

The **Reason** is that this code is optimised for fast computation. Manly due to the utilisation of GPUs which offer high parallelism. *Don't bother trying to implement a faster version*.

![](imagesi/paste-19.png){fig-align="center" width="9cm" height="1.7cm"}

![](imagesi/paste-20.png){fig-align="center" width="7cm" height="4.6cm"}

![](imagesi/paste-21.png)

## Morals of AI

### Gender Shades

Gender detecting algorithm is trained on a highly biased dataset, which leads to different results, depending on the target. [Read More](http://gendershades.org/)

![](imagesi/paste-39.png){fig-align="center" width="8cm" height="2.7cm"}

::: callout-caution
## Beyond Average Test-Set Performance

Even if a test-set is a well representation of the real world, which will lead to good average accuracy. The network can still do badly on minority group test sets.

**Good average performance can mask poor performance of specific cases**

![](imagesi/paste-45.png){fig-align="center" width="8cm" height="3cm"}
:::

### Reknognition

Facial Recognition algorithm wrongly matches government members to mugshots from a criminal database. The algorithm had **5% false positives** which isn't to bad, but when deployed states big issues of wrongly accusing innocent people. [Read More](https://www.aclu.org/news/privacy-technology/amazons-face-recognition-falsely-matched-28)

# Machine Learning Concepts

**Machine Learning == Function Approximation**

![](imagesi/paste-1.png){fig-align="center" width="6cm" height="3.3cm"}

![](imagesi/paste-4.png){fig-align="center" width="7cm"}

## Types of Learning

![](imagesi/paste-2.png)

![](imagesi/paste-3.png){fig-align="center" width="9cm" height="3.5cm"}

### Self-Supervised Learning

From data **without labels** we can learn the structure of the data itself, there are several approaches, the basic idea is dimensionality reduction:

-   K-Means Clustering
-   Principal Component Analysis (PCA)
-   Butterfly-Network (Autoencoder)
-   ...

### Reinforcement Learning

Requires no pre-collected data, it learns from experience by playing a game over and over again. **Agent** wants to choose actions that will give it higher scores

![](images/paste-39.png){fig-align="center" width="7cm"}

::: callout-caution
## RL Reservations

-   Inefficient with data
    -   AlphaGo learned from playing 100mio times
    -   Human Go champion only played 50k times
-   Sensitive to small perturbations in the environment
:::

![](images/paste-41.png){fig-align="center" width="7cm"}

#### Markov Decision Process (MDP)

-   States $S$ - set of possible situations in the world
-   Actions $A$ - set of different actions an agent can take
-   Transistion function $T(s,a,s')$ - returns **possibility** of transitioning to state $s'$ after taking action $a$ in state $s$
    -   $T(s,a,s')=P(s_{t+1}=s'|_{s_t=s,a_t=a})$
-   Reward function $R(s,a,s')$ - returns reward received for transition to state $s'$ after action $a$ on state $s$ is taken

In the *Markov Decision Process* history doesn't matter, only the current state.

![](images/paste-40.png){fig-align="center" width="8cm" height="2.4cm"}

Goal: maximise sum of discounted future rewards $G_t$, "return"

$$
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots=\sum_{k=0}^\infty \gamma^kR_{t+k+1}
$$

$\gamma$ is the discount factor - value soon rewards not later ones. **Rewards exponentially decay the later they are received**.

#### Policies

Policy Functions say what action should a agent take in a given state

$$
\pi:S\rightarrow A
$$

Output: $\pi(s)=a$ means in state $s$, take action $a$.

**The goal of RL is to learn an optimal policy** $\pi$**, that maximises future cumulative reward**

#### Value Iteration (V-Function)

.

![](images/paste-42.png)

-   **Discrete state spaces** can be represented by a lookup table

-   **Continuous or very large discrete state spaces** will need to be represented by a neural network

#### Q-Function

.

![](images/paste-43.png)

![](images/paste-44.png)

::: callout-note
## Q-Function in terms of V-Function

$$
Q^\pi(s,a)=E[R(s,a,s')+\gamma V^\pi(s')]
$$

$$
Q^\pi(s,a)=\sum_{s'\in S}P(s'|_{s,a})[R(s,a,s')+\gamma V^\pi(s')]
$$
:::

::: callout-tip
## Learn V\* and Q\*

![](images/paste-45.png){fig-align="center" width="8cm" height="2.6cm"}

**Example:**

$T$ is known as $1/3$ in forward/left/right when stepping forward.

![](images/paste-46.png)
:::

#### Deep-Q Learning

If the Transition $T$ or Reward $R$ are not known

![](images/paste-47.png){fig-align="center" width="7cm" height="3cm"}

![](images/paste-48.png)

::: callout-warning
## The limits of tabular learning

The above approach still uses a look up table to determine what action on what field to take. To work with big (continuous environments), we use neural networks as an approximator

![](images/paste-49.png){fig-align="center" width="7cm"}

``` python
# Weights for Q-Network 
Q = tf.Variable(tf.random.uniform([16,4],0,0.01)) 
# Q-value function
def qVals(inptSt):
    oneH = tf.one_hot(inptSt,16) 
    qVals = tf.matmul([oneH],Q) 
    return qVals
# argmax over q-values to get estimated best action 
action = tf.argmax(qVals(st),1)
```

For training such a network, we replace the Q-values LUT with the Network

``` python
Q_values = qVals(st) 
if np.random.rand(1) < epsilon:
    act = env.action_space.sample()
else:
    act = tf.argmax(Q_values) 
nst, rwd, done, _ = env.step(act) 
nextQ = qVals(nst)
loss = tf.reduce_sum(tf.square(Q_values - rwd+gamma*nextQ)) optimizer.apply_gradients(tape.gradient(loss,Q),Q)
```
:::

## Types of Problems

## Maschine Learning Pipeline

![](imagesi/paste-6.png)

### Dataset

*Annotated Datasets* like [MNIST](https://yann.lecun.com/exdb/mnist/) (Handwritten digits).

### Preprocessing

Split the dataset into **Train, Validation, and Test sets**

![](imagesi/paste-7.png){fig-align="center" width="8cm"}

### Train Model

1.  **Initialization**: Set all weights $w_i$ to 0.
2.  **Iteration Process**:
    -   Repeat for $N$ iterations, or until the weights no longer change:
        -   For each training example $\mathbf{x}^k$ with label $a^k$:
            1.  Calculate the prediction error:
                -   If $a^k - f(\mathbf{x}^k) = 0$, continue (no change to weights).
            2.  Otherwise, update each weight $w_i$ using: $$ w_i = w_i + \lambda \left( a^k - f(\mathbf{x}^k) \right) x_i^k $$
    -   where $\lambda$ is a value between 0 and 1, representing the learning rate.

## Optimizing with Gradient Descent

### Loss Function

Function $L$ which measures how "wrong" a network is. We want our network to answer right with **high probability**.

![](imagesi/paste-11.png){fig-align="center" width="9cm" height="2.9cm"}

To get a probability for **binary classification**, we introduce a **probability layer**. One of the possible function is **Softmax**

$$
p_j=\frac{e^{l_j}}{\sum_k e^{l_k}}
$$

For every output $j$ it takes every logit (output of network before activation/probability is applied) $l_j$ in the exponent to ensure positivity. Dividing it by the sum of all logits ensures that $\sum_kp_k=1$.

To get the loss $L$ we apply a loss-function, *low probability* $\rightarrow$ *high loss*. We use **Cross Entropy Loss**

![](imagesi/paste-13.png)

### Gradient Descent

$\Delta w_{j,i}=-\alpha\frac{\partial L}{\partial w_{j,i}}$

| $\alpha$: learning rate *(typically 0.1-0.001)*
| $L$: loss function
| $w_{j,i}$: one single weight

To compute $-\alpha\frac{\partial L}{\partial w_{j,i}}$ use the chain rule

![](imagesi/paste-12.png)

``` python
## Backpropagation on batch learning
# y = expected - (f(x)>0)
labels_OH = np.zeros((labels.size, self.num_classes), dtype=int)
labels_OH[np.arange(labels.size),labels] = 1  # One-Hot encoding
predictions = np.argmax(outputs, axis=1)
predictions_OH = np.zeros_like(outputs)
predictions_OH[np.arange(outputs.shape[0]), predictions] = 1
y = labels_OH - predictions_OH
# db = y*1
gradB = np.mean(y, axis=0)   # average over batch
# dW = y*x
y = y.reshape((outputs.shape[0],1,self.num_classes))
inputs = inputs.reshape((outputs.shape[0],self.input_size[0]*self.input_size[1],1))
dW = inputs*y
gradW = np.mean(dW, axis=0)  # average over batch
```

### Stochastic Gradient Descent (SGD)

Train a network on **batches**, small subsets of training data.

``` python
# Stochastic Gradient Descent
for start in range(0, len(train_inputs), model.batch_size):
    inputs = train_inputs[start:start+model.batch_size]
    labels = train_labels[start:start+model.batch_size]
    # For every batch, compute then descend the gradients for the model's weights
    outputs = model.call(inputs)
    gradientsW, gradientsB = model.back_propagation(inputs, outputs, labels)
    model.gradient_descent(gradientsW, gradientsB)
```

-   Training process is *stochastic* / *non-deterministic*: batches are a random subsample.
-   The gradient of a random-sampled batch is a unbiased estimator of the overall gradient of the dataset.
-   Pick a large enough batch size for s*table updates*, but small enough to *fit your GPU*

::: callout-warning
## Stuck Gradients

When the gradients get low or there is a local minima, SGD can get **Stuck**.

![](imagesi/paste-69.png){fig-align="center" width="6cm"}
:::

### Adaptive Momentum Estimation (Adam)

Two moments: SGD momentum and squared gradients. Also uses an exponentially decaying average. Fast and almost always the best, very little need to hyperparameter tune learning rate.

See [Notebook](https://colab.research.google.com/drive/1x2jN4uLMpd3VLVAA435bfwlE3s55sNti)

## Automatic Differentiation

To avoid having to recalculate the whole chain every time a new layer is added, we use *automatic derivation*. There are several options:

### Numeric differentiation

-   $\frac{df}{dx}\approx\frac{f(x+\Delta x)-f(x)}{\Delta x}$
-   Called *finite differences*
-   Easy to implement
-   Arbitraritly inaccurate/unstable

### Symbolic differentiation

-   $\frac{dx^2}{dx}=2x$
-   Computer does algebra and simplifies expressions
-   Very exact
-   Complex to implement
-   Only handles static expressions

### Automatic differentiation

-   Use the chain rule at runtime
-   Gives exact results
-   Handles dynamics
-   Easier to implement
-   Can't simplify expressions

#### Forward Mode Autodiff

Every node stores its `(value, derivative)` in a tuple, called **dual numbers**. To compute the overall derivative, each derivative can be chained up. This is implemented via **Overloading**, every function / operator has multiple definitions based on the types of the arguments. ML-Framwork functions work on these tuples.

![](imagesi/paste-14.png)

**Time Effect**: $O(N*M)$ time, $O(1)$ memory, with $N$ = number of inputs, with $M$ = number of nodes

::: callout-note
## Issue w/ forward mode

![](imagesi/paste-15.png)
:::

#### Reverse Mode Autodiff

First, run the function to produce the graph, then compute the **derivatives backward**.

![](imagesi/paste-16.png){fig-align="center" width="7cm"}

-   Analog to the forward mode: overload math functions/operators
-   Overloaded function return *Node* objects
-   Overloaded functions build compute graph while executing
-   After forward pass, the operations are recorded
-   The backwards pass walks along the graph and computes the derivatives
-   **Time Effect**: $O(M)$ time, $O(M)$ memory, with $M$ = number of nodes

#### Fan-Outs (Reverse)

The way to handle fan-out is to **add** the derivatives of the fanned-out nodes through replication $r(x)$.

![](imagesi/paste-17.png)

## Diagnosis Problems

### Overfitting

Training on a complex dataset can lead to **overfitting** ([PTB](https://paperswithcode.com/dataset/penn-treebank)is a language dataset).

![](imagesi/paste-40.png){fig-align="center" width="8cm"}

### Regularization

This approach **modifies the loss** through adding a additional term to our existing loss function.

![](imagesi/paste-44.png)

Regularization can be applied to certain layers on Keras through `tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(lambda), activation='relu')`.

# Deep Learning Concepts

::: callout-caution
## Common Misconception

**Deep Learning != AI**, Just because deep learning algorithms are used doesn't mean there is any intelligence involved.

**Deep Learning != Brain**, Modern deep nets don't depend solely on *biologically mimiced neural nets* any more. A fully connected layer represents such a neural net the closest.

**Deep Learning ==**:

1.  *Differentiable functions*, composed to more complex diff. func.
2.  A deep net is a differentiable function, some inputs are *optimizable parameters*
3.  Differentiable functions produce a computiation graph, which can be traversed backwards for *gradient-based optimization*
:::

## Multi-Dimensional Arrays & Memory Models

### Vectorized Operations

For efficient operation, use **Matrices**.

![](imagesi/paste-18.png)

## Neural Networks

### Perceptron

![](imagesi/paste-8.png){fig-align="center" width="8cm"}

**Predicting with a Perceptron**:

1.  Multiply the inputs $x_i$ by their corresponding weight $w_i$
2.  Add the bias $b$
3.  **Binary Classifier**, greater than 0, return 1, else return 0

$$
f_{\Phi}(\mathbf{x}) =\begin{cases} 1, & \text{if } b + \mathbf{w} \cdot \mathbf{x} > 0 \\0, & \text{otherwise}\end{cases}
$$

::: callout-important
## Parameters

**Weights**: "importance of the input to the output"

-   Weight near 0: Input has little meaning to the output
-   Negative weight: Increasing input $\rightarrow$ decreasing output

**Bias**: "a priori likelihood of positive class"

-   Ensures that even if all inputs are 0, there is some result
-   Can also be written as a weight for a constant $1$ input

\begin{align*}
[ x_0, x_1, x_2, \dots, x_n ] \cdot [ w_0, w_1, w_2, \dots, w_n ] + b \\
= [ x_0, x_1, x_2, \dots, x_n, 1 ] \cdot [ w_0, w_1, w_2, \dots, w_n, b ]
\end{align*}
:::

**Multi-Class Perceptron**

![](imagesi/paste-9.png){fig-align="center" width="8cm"}

**Biary Classifier**: Only one output can be active $\hat{y} = \text{argmax}\left(f\left(x^k\right)\right)$, thus the update terms are

$$
\Delta w_i=
\begin{cases}
0, &\text{for }a^k=\hat{y} \\
-x_i^k, &\text{for }\hat{y}=1,\space a^k=0 \\
x_i^k, &\text{for }\hat{y}=0, \space a^k=1
\end{cases}
$$

### Multi-Layer

Through adding hidden layers we can make bigger networks and add more states to the algorithm.

![](imagesi/paste-22.png)

The size of these **hidden layers** are defined by the **hyperparameter**. These define the configuration of a model and are set before training begins. *Rule of Thumb*: Make hidden layers the same size as the input, then start to tweak to see the effect. If you have more time and money, [check this](https://en.wikipedia.org/wiki/Hyperparameter_optimization).

![](imagesi/paste-27.png){fig-align="center" width="7cm"}

::: callout-note
## Universal Approximation Theorem

Remarkably, a one-hidden-layer network can actually represent any function (under the following assumptions):

-   Function is continuous
-   We are modeling the function over a closed, bounded subset of $\mathbb{R}^n$
-   Activation function is sigmoidal (i.e. bounded and monotonic)

![](imagesi/paste-28.png)
:::

::: callout-warning
## Stacking Linear Layers isn't Enough

When simplifying the linear equation we get

$$
\sigma\left([\begin{smallmatrix} w_2 & b_2 \end{smallmatrix}]
\left([\begin{smallmatrix} w_1 & b_1 \end{smallmatrix}]\left[\begin{smallmatrix} x\\1 \end{smallmatrix}\right]\right)\right) =
\sigma\left([\begin{smallmatrix} w_{12} & b_{12} \end{smallmatrix}]\left[\begin{smallmatrix} x\\1 \end{smallmatrix}\right]\right)
$$

Which is exactly the same as just one layer again, we need **activation**.
:::

### Activation Functions

We introduce a **nonlinear** layer

![](imagesi/paste-10.png){fig-align="center" width="7cm"}

A activation function binds network outputs to a particular range. In the last layer this can be used to restrict the range, for example *age is strictly positive*.

Futher PyTorch activation functions can be found [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).

#### Sigmoid

.

![](imagesi/paste-23.png)

#### Tanh

.

![](imagesi/paste-24.png)

::: callout-warning
## Vanishing Gradient

The problem with **Sigmoid** and **Tanh** is that the further away the parameters get from zero, the smaller is the gradient. Thus the network stops learning at these points. When **stacking layers** the issue gets even more severe.
:::

#### ReLU

.

![](imagesi/paste-25.png)

::: callout-warning
## Dead ReLU

Because the negative part fed into the activation will result in a $0$ output. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again.
:::

#### Leaky ReLU

To tackle a possible *dead ReLU* issue, we use a tiny positive slope for negative inputs.

![](imagesi/paste-26.png)

### Convolution

Convolution is like a *"partially connected"* layer. Only certain inputs are connected to certain output pixels.

To introduce **translational invariance** $f(T(x))=f(x)$ we apply convolutions. These are "Filters" which highlight different structures, the following network makes sense from the structures, not the pixels itself. Main application: **Computer Vision**.

![](imagesi/paste-29.png){fig-align="center" width="7cm" height="2.2cm"}

::: callout-note
## Hyperparameters

There are 4 hyperparameters for the convolution

-   Number of filters, $n$
-   Size of these filters, $n$
-   The Stride, $s$
-   Amount of padding, $p$

![](imagesi/paste-34.png){fig-align="center" width="8cm"}

We can calculate the output size through

$$
w'=\frac{w-f+2p}{s}+1,\space h'=\frac{h-f+2p}{s}+1, \space d'=n
$$

For **VALID** padding $p=0$, for **SAME** padding $p$ is chosen so output is same
:::

``` python
# Execute manual Convolution
# Should be of shape (batch_sz, 32, 32, 3) for CIFAR10 
inputs = CIFAR_image_batch
# Sets up a 5x5 filter with 3 input channels and 16 output channels 
self.filter = tf.Variable(tf.random.normal([5, 5, 3, 16], stddev=0.1))
# Convolves the input batch with our defined filter 
conv = tf.nn.conv2d(inputs, self.filter, [1, 2, 2, 1], padding="SAME")
```

The inputs to `tf.nn.conv2d(...)` are:

-   `input` = \[batchSz, input_height, input_width, input_channels\]
-   `filter` = \[f_height, f_width, in_channels, out_channels\]
-   `strides` = \[batch_stride, stride_along_height, stride_along_width, stride_along_input_channels\]
-   `padding` = either `'SAME'` or `'VALID'`

Tipically there are several convolutional layers and then a fully connected layer. This can be achieved through flattening a layer `flat = tf.reshape(conv, [conv.shape[0],-1])`.

![](imagesi/paste-36.png)

#### Stride

The distance we slide a filter on each iteration is called **stride**. With a bigger stride, you compress a same size input into a smaller output. This decreases the image resolution controlled, **Downsampling**. The filters are **Kernels** and are made of **learnable parameters**.

![](imagesi/paste-30.png)

##### Computation

The image is flattened and the kernel is unrolled into a bigger matrix. This leads to a normal **matrix/vector multiplication**

![](imagesi/paste-56.png)

#### Fractional Stride

For **deconvolutions** you can also use fractionally-strided convolutions (here $\frac12$ stride):

![](imagesi/paste-55.png){fig-align="center" width="7cm" height="3.9cm"}

##### Computation

Same as with the convolution, we flatten all matrices into vectors/matrices but now we **transpose** the kernel matrix, which gives us the **de-convolution**:

![](imagesi/paste-57.png){fig-align="center" width="5cm"}

::: callout-caution
## Checkerboard Artifacts

THe transpose convolution causes [artifacts in output images](https://distill.pub/2016/deconv-checkerboard/) because some pixels get written more often than others (at the overlaps, which occur in a line).

![](imagesi/paste-58.png){fig-align="center" width="6cm" height="1.5cm"}

*Prevention*:

1.  Upsampling using nearest neighbour interpolation

![](imagesi/paste-59.png){fig-align="center" width="7cm"}

2.  Perform a convolution with `'SAME'` padding on upsampled image

![](imagesi/paste-60.png){fig-align="center" width="8cm"}

``` python
# Layer to upsample the image by a factor of 5 in x and y using nearest 
# neighbor interpolation
tf.keras.layers.UpSampling2D(size=(5, 5), interpolation=’nearest’)
# Do a convolutional layer on the result 
tf.keras.layers.Conv2D(filters = 1, kernel_size = (10,10), padding = “SAME”)
```
:::

#### Filter Banks

Futhermore, use several kernels per image, this block of kernels is a **filter bank**. The output is then a **mutli-channel** image. Multiple filters are able to extract *different features* of the image.

![](imagesi/paste-31.png){fig-align="center" width="8cm"}

#### Padding

To not loose resolution through a convolution, the original image has to be extended, **padded**. There are two convolution options, **VALID**, which is without padding, or **SAME** which is padding so that the output size is same as the input size.

![](imagesi/paste-32.png){fig-align="center" width="8cm" height="4cm"}

#### Bias

As with other layers, a **Bias** can be added to the convolutional layer

![](imagesi/paste-35.png){fig-align="center" width="8cm"}

This can be done through `tf.nn.bias_add(value, bias)`. When using keras layers, a bias is included by default `tf.keras.layers.Conv2D(filters, kernel_sz, strides, padding, use_bias = True)`.

### Pooling

Pooling keeps track of the regions with the highest activation, indicating object presence, also lowers the resolution in a controllable way.

![](imagesi/paste-37.png){fig-align="center" width="8cm"}

### Invariances

The translational variance is largely eliminated with the introduction of convolutions, this can be further improved by introduction of [antialising](https://arxiv.org/pdf/1904.11486).

There are further invariances, which can hurt a CNNs performance. CNNs don't do to good on these, for good performance, lots of training is needed.

![](imagesi/paste-38.png){fig-align="center" width="7cm"}

## Recurrent Networks

::: callout-warning
## Problems of N-gram Model

The issue with N-gram models is, that they're **not flexible**, each additional word means more weights to train. That leads to a fixed input size.

![](images/paste-7.png){fig-align="center" width="8cm" height="2.3cm"}
:::

To takle this we introduce **Recurrent Networks (RNN)**, based on Bigram models in a recurrent (not recursive) connection.

![](images/paste-8.png){fig-align="center" width="8cm" height="3.1cm"}

A **RNN** cell consists of two Feed Forward (FF) layers, one to compute the next state and one to compute the output.

![](images/paste-9.png){fig-align="center" width="6cm"}

$$
s_t=\rho((e_t, s_{t-1})W_r+b_r)
$$

$$
o_t=\sigma(s_tW_o+b_o)
$$

*Note*: $s_0$ is typically a zero vector.

### Training RNNs

Because the gradients for $o_t$ depend on $x_t$ and all previous inputs, we **backpropagate through time**. Because we reuse the same block over and over again, the gradient accumulates over time. The size of these iterations can be used as a hyperparameter $\textcolor{orange}{\text{window}_{sz}}$ .

![](images/paste-10.png){fig-align="center" width="8cm"}

### Long Short Term Memory (LSTM)

::: callout-caution
## Short Term Memory of RNNs

When there is a long sequence of words, we must somehow make shure, that information from the beginning isn't lost through iterating over several blocks.

![](images/paste-11.png){fig-align="center" width="7cm" height="2.5cm"}

The information will slowly fade away over the recurrent convolutions.
:::

To handle this, we have the **LSTM**-Cell, consisting of three modules

![](images/paste-12.png){fig-align="center" width="10cm" height="4.1cm"}

#### Forget Module

Filters what gets allowed into the LSTM-cell from the last state *(e.g. gender pronouns are coming in* $c_{t-1}$*, a new subject is seen* $x_t$*,* $\rightarrow$ *forget old pronouns).*

Forgetting is handled by a point wise multiplication with a mask, it zeros out part of the cell state.

![](images/paste-13.png){fig-align="center" width="6cm"}

#### Remember Module

We can save information into the previously emptied slots in the cell state. First a point wise multiplication is used to decide what to remember, then this selective memory to the cell state

![](images/paste-14.png){fig-align="center" width="9cm" height="2.2cm"}

**Solution**: Cell state never goes through a fully connected layer $\rightarrow$ No mix-up of information.

#### Output Module

Provides path for short-term memory $h_t$ to temporarily acquire from the long-term cell state.

## Sequential Networks

## Latent Space

**Latent Space** is a compact representation, representing the input in a lower dimension. This is used for **Self-Supervised Learning**.

Represent the data with fewer dimensions, although data might exist in high dimensional space, it actually may exist along a lower dimensional subspace (e.g. 2D-Plane in 3D-Space, Line in 2D-Space), **Dimensionality Reduction.**

We do this for **smaller dataset footprint (memory)**, more efficient search through **nearest neighbour algorithms**, many **clustering algorithms behave better** in lower dimensions, Easier to **visualize**.

### Principal Component Analyis (PCA)

![](imagesi/paste-46.png){fig-align="center" width="9cm"}

Given a dataset $D$ of dimension $n$ and a target dimension $m\leq n$, find $m$ vectors in $\mathbb{R}^n$ along which $D$ has the highest variance. $m$ are the **principal components**.

How: Find the direction of maximal variation, project onto this vector, repeat $m$ times.

*Example with MNIST:*

![](imagesi/paste-47.png){fig-align="center" width="8cm"}

::: callout-caution
## Limitations of PCA

PCA can't figure out **non-linear** projections from $\mathbb{R}^2$ to $\mathbb{R}^1$.

![](imagesi/paste-48.png){fig-align="center" width="7cm" height="2cm"}
:::

### Butterfly-Network (Autoencoder)

Because we don't have labels, we copy the first part of the network and inverse it. The loss is then calculated through

$$
L(x,\bar{x})=(x-\bar{x})^2\qquad\text{squared error loss}
$$

![](imagesi/paste-49.png){fig-align="center" width="9cm"}

After training the network with the unlabelled data, the second part can be cut off and a binary layer be added. Now fine tune the last layer on **labelled data through supervised learning**.

![](imagesi/paste-50.png){fig-align="center" width="7cm" height="2.9cm"}

### Convolutional Autoencoder

The same approach can be applied to convolutional networks. But **convolutional matrices are expensive**, thus we just swap the forward and backwards pass code for the **deconvolution**.

![](imagesi/paste-53.png){fig-align="center" width="8cm" height="1.9cm"}

This can be done in [Tensorflow](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/conv2d_transpose)

![](imagesi/paste-54.png){fig-align="center" width="10cm"}

### Autoencoder Applications

#### Anomaly / Novelty Detection

.

![](imagesi/paste-52.png)

#### Others

-   Transformer
-   Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept)
-   Subspace, correlation-based, and tensor-based outlier detection for highdimensional data
-   One-class support vector machines
-   Replicator neural networks, autoencoders, and long short-term memory neural networks
-   Bayesian Networks
-   Hidden Markov models (HMMs)
-   Cluster analysis-based outlier detection
-   Deviations from association rules and frequent itemsets
-   Fuzzy logic-based outlier detection;

## Transfer Learning

Taking a pre-trained network of a similar domain and adjust the last layers.

![](imagesi/paste-70.png){fig-align="center" width="8cm"}

-   Saves training time & money
-   Network is more likely to generalise

### Fine Tuning

One can switch out the **last layer** and add a different classification

![](imagesi/paste-71.png){fig-align="center" width="7cm"}

Or also switch add/replace a fully connected layer if the differences in the domain are bigger

![](imagesi/paste-72.png){fig-align="center" width="7cm"}

### Zero Training

The last layer can be replaced by a *Logistic Regression* or *Nearest Neighbor Classifier.* The *transferred network* is then just a feature extraction pipeline. Use a database with known targets and classify new data through searching for nearest neighbour.

![](imagesi/paste-73.png){fig-align="center" width="8cm" height="3.3cm"}

### Few-Shot Learning

When there are only **few example of a sample** to be detected.

**Siamese Networks** where the sample and the test go through the same network. Then they are compared in the latent space.

![](imagesi/paste-74.png){fig-align="center" width="8cm"}

Nominate or Generate a **representative** from each class

![](imagesi/paste-75.png){fig-align="center" width="5cm" height="3.7cm"}

## Training Methods and Tricks

### Early Stopping

If one stops training when the testing loss is starting to rise, at least the loss won't get bigger

``` python
# Pseudo Code Early Stopping
curr_test_loss = inf 
for i in range(n_epochs):
    train model()
    new_test_loss = model.get_test_loss() 
    if new_test_loss > curr_test_loss:
        break
    else:
        curr_test_loss = new_test_loss
```

### Reduce Parameters

Reducing parameters, means less possibilities to learn or even memorize a dataset.

![](imagesi/paste-41.png){fig-align="center" width="8cm"}

Reducing parameters can mean...

-   ... reducing layer size
-   ... decrease number of channels in a convolution
-   ... decrease number of layers

::: callout-tip
## Reducing Parameters

Can also be used to check if parts of a network **are actually needed** $\rightarrow$ remove part $\rightarrow$ retrain model $\rightarrow$ if it behaves the same, it wasn't needed.

Requires obnoxious tuning of hyperparameters.
:::

### Data Agumentation

Generate **random variations** on your training data.

![](imagesi/paste-42.png)

### Dropout

*Make it harder for the network*. In a single training pass, the output of randomly selected nodes from each layer are set to $0$. The nodes that drop out are **different each pass**. This builds **Resillience**. During testing all nodes are active again.

![](imagesi/paste-43.png){fig-align="center" width="7cm" height="3.8cm"}

Dropout can be handled with Keras through `tf.keras.layers.dropout(rate)`, where `rate`is a *hyperparameter* between $[0,1]$. `rate=0.5` is drop $\frac12$, keep $\frac12$. `rate=0.25` is drop $\frac14$, keep $\frac34$.

### Skip Connections / Residual Blocks

::: callout-caution
## Vanishing Gradients

The deeper a net gets (more layers) the more learnable parameters are present. This leads to vanishing of the gradient throughout the network

![](imagesi/paste-61.png){fig-align="center" width="5cm" height="3.1cm"}
:::

To mitigate the *vanishing gradient problem*, we use **Redidual Blocks**

![](imagesi/paste-62.png){fig-align="center" width="7cm"}

The output of each layer is the identity + some deviation (residual) from it. It allows the gradient to flow through two pathways. **Significantly stabilises training of very deep networks.**

### Dense Connectivity

To reduce redundancy, **dense connectivity** can be used. for this a network with fewer parameters has increased amounts of connections

![](images/paste-32.png){fig-align="center" width="8cm" height="4cm"}

Example:

-   ResNet: 18 layers, 11.7M parameters, 30% validation error
-   DenseNet: 121 layers, 8M parameters, 25% validation error

[Skipping connections and connecting densely is improving the loss surface (better trainability)](https://www.cs.umd.edu/~tomg/projects/landscapes/)

### Batch Normalisation

The idea is to stabilise inputs, which should lead to **faster training**. This is done through normalisation of the layers' inputs by re-centring and re-scaling.

A **normalisation layer** $BN$ is added after a fully connected or a convolution, before the non-linear activation. In*Tensorflow* this is done with `tf.keras.layers.BatNormalization(input)`.

![](imagesi/paste-63.png)

![](imagesi/paste-64.png){fig-align="left" width="7cm" height="3cm"}

### Checkpointing

::: callout-warning
## Training is Expensive

Training takes a while and sometimes you want to “save” data for use in future instances.

**Checkpointing** allows you to save your Tensorflow model! No need to retrain every time your run your program. Fast prediction. Export your trained weights for use in other applications
:::

![](imagesi/paste-65.png){width="7cm" height="3cm"}

Example: (more [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/checkpoint.ipynb))

![](imagesi/paste-66.png){width="8cm" height="3.6cm"}

### Xavier Initialisation

::: callout-warning
## Bad Initialization

With certain activation functions, it's possible to get bad initialisations. For example with the sigmoid function we have the issue of linearity around $x\approx0$ and the flatness / low gradient at $x>|4|$.

![](imagesi/paste-67.png){fig-align="center" width="6cm"}

One solution is to use activations which don't have these issues, like **ReLU**.

Event then we want to keep values in the same range when they flow through a network. Values that drastically fluctuate in magnitude can lead to **numerical instability** $\rightarrow$ *slow convergence*.

Consider a weight matrix $W$ of size $m × n$ :

-   One entry $y_i$ of the product $Wx$ is: $y_i = W{i,1}x_1 +\cdots + W{i,j}x_j + \cdots+ W_{i,n}x_n$
-   If $n$ increases, then the magnitude of $y_i$ would also tend to increase
-   If $m$ increases, then the output vector $Wx$ would have a larger dimension
-   As the output becomes the input for the next layer, the next layer would add up in terms

We want the magnitude of weights to be **inversely proportional** to $m$ and $n$.
:::

To tackle this issue, we use the **Xavier Initialization**, we calculate the *standard deviation* on each layer $i$ new:

$$
\sigma_i=\sqrt{\frac2{n_i+m_i}}
$$

### Keras

Tensorflow code frequently gets terribly cumbersome, [**Keras**](https://keras.io/) **shortens the amount of code** you need to write through higher-level APIs for constructing, training, and evaluating models.

### Tensorboard (Visualization)

A powerful visualisation, logging, and monitoring tool designed to be integrated with Tensorflow (although you can technically use it with any Python code).

[Check the Demo](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master%20/docs/get_started.ipynb)

# Deep Learning Problems, Models & Research

## Computer Graphics

### Deepfakes

1.  Use face detector to identify the face in all frames $\rightarrow$ Crop out and rectify
2.  Train NN to transform aligned images of person A's to person B's face with **same facial expressions**
3.  Merge the person B face onto the original person A frame
    -   Merging incorporates: colour correction, seam blending

![](images/paste-30.png)

#### CycleGAN

To train a network to swap faces (**or any other domain**), we use a CycleGAN. This trains two Autoencoders and a Discriminator to:

1.  Generate a fake image in Domain B
2.  Check with discriminator if it looks "real"
3.  Generate a reconstructed image from the fake image into Domain A
4.  Calculate the loss on how different the reconstructed image is to the original

![](images/paste-31.png)

#### Deepfake detection

There are several ways to detect deepfakes:

-   Train CNNs on real videos + fakes to detect fakes
-   Look for optical flow aberrations
-   Look for heart rate aberrations
-   Look for camera signatures

The issue is, that the deepfakes evolve with every new detection techniques (generator / discriminator!).

But there is a deeper issue, not all deepfakes are bad (parody, human aid, ....). On the other hand, misinformation and propaganda doesn't need deep fakes to be spread. Solutions(?): more human-in-the-loop, better-paid moderators, education, ...

### Denoising

**Autoencoder** networks can be trained to denoise images. The training is easy, as you just add the noise to original images.

![](imagesi/paste-51.png){fig-align="center" width="8cm" height="2.3cm"}

### Attention

With attention we can transfer information from the input to the output. We pass a weighted sum of encoder states to the decoder.

![](imagesi/paste-76.png){fig-align="center" width="7cm"}

The attention can also be visualised

![](imagesi/paste-83.png){fig-align="center" width="8cm"}

## Computer Vision \| 3D Reconstruction

### Application:

-   Medical Imaging and Sensing (e.g. Spectral Photon-Counting CT)

![](images/paste-58.png){fig-align="center" width="8cm"}

-   Agriculture (e.g. Plant Scanning)

Challenges: Harsh lighting conditions, Small plant features, SFM unstable down the whole row, Leaves (occlusion, movement), Scans from both sides must be matched

![](images/paste-59.png){fig-align="center" width="7cm"}

### 3D Reconstruction Classical

#### Analytical / Iterative

1.  Generate a random scene and get a projection of this scene
2.  Compare artificial projection to the actual scan
3.  Adjust the scene to better fit the actual scan
4.  Go back to 1. - iterate till artificial and actual scan look the same

![](images/paste-60.png){fig-align="center" width="8cm" height="2.1cm"}

#### Stereo Matching

![](images/paste-61.png){fig-align="center" width="8cm"}

#### Gaussian Splatting

![](images/paste-64.png){fig-align="center" width="8cm" height="3.8cm"}

#### Shape-Radiance Amiguity

![](images/paste-65.png){fig-align="center" width="7cm"}

### Differentiable rendering (NeRF)

-   Inputs: Set of aligned images
-   Render images using differentiable model
-   Update model from different viewpoints until convergence
-   Very flexible models to allow view of dependent colous

![](images/paste-62.png){fig-align="center" width="8cm"}

**Radiance Field**: Network gives a radiance field at intermediate points over a ray. THese points are integrated and compared to the image pixel to a corresponding image. $\rightarrow$ optimize network so that these results are right.

Ask the model: Give me the radiance field at intermediate points on a ray.

![](images/paste-63.png){fig-align="center" width="8cm" height="3.7cm"}

## Generative- \| Diffusion-Models

In comparison to discriminative models, the generative model doesn't have a clear decision boundary. Therefore a image which shows non-sense, isn't classified into a random class, but shown as very low likely hood on every class.

![](images/Xrmqg.png){fig-align="center" width="8cm"}

### Generative Modeling

Generative models are trained to generate a data distribution $\hat{p}(x)$ from a noise input (uniform, gaussian, ...). The loss is calculated in reference to the actual data distribution of the ground truth dataset.

![](images/paste-16.png){fig-align="center" width="8cm" height="2.8cm"}

**Use Cases**:

-   Text-to-Image Generation, $p(image|text_caption)$
-   Super Resolution, $p(image|low_res)$
-   Class-Conditional Generation, $p(image|class_label)$

### Generative Adversarial Network (GAN)

A **GAN** consists of a **Generator** and a **Discriminator** which try to outwork each other. They are trained in the same loop.

#### Training the Discriminator

The Discriminator want's to distinguish the real from the fake samples as good as possible and is trained for that.

![](images/paste-17.png){fig-align="center" width="9cm" height="2.1cm"}

#### Training the Generator

The Generator want's to generate images which are classified as real as possible. The Generator updates it's own weights to generate a more real looking image.

![](images/paste-18.png){fig-align="center" width="9cm" height="2.5cm"}

#### GAN training dynamics

GAN training losses don't typically converge to zero because GANs involve a balance between the two competing networks generator and discriminator.

**Attention**: There can be issues with instability in the training of GANs.

![](images/paste-19.png){fig-align="center" width="8cm"}

::: callout-warning
## Mode Collapse

A issue arises if the generator figures out a output that looks real, and continues to only show this as it's only output.

![](images/paste-20.png){fig-align="center" width="8cm"}
:::

### Evaluate GANs

Evaluating GANs can be hard, as we train the discriminator and the generator. A simple way to do it is to **Eyeball**, a human looking at the data and saying if it looks better or not. This isn't very scientific.

### Variational Autoencoder (VAE)

A **VAE** uses the a autoencoder as a basis, but the latent space gets sampled randomly before it's feed back into the decoder.

-   The random sampling should be designed to produce random points in the latent space that are close to the output of the encoder
-   Nearby points in the latent space should decode similar images

![](images/paste-21.png){fig-align="center" width="8cm"}

Because "random sampling" is non-differentiable, we introduce a point wise multiplication of the encoder output $\sigma$ and a uniform distribution $\varepsilon = N(0,1)$ (**seed**). This will be added to the output $\mu$ of the encoder, giving us the input for the decoder

$$
z=\mu+\varepsilon \cdot \sigma
$$

![](images/paste-22.png){fig-align="center" width="8cm"}

It can also be shown as

![](images/paste-25.png){fig-align="center" width="7cm" height="2.2cm"}

#### Latent Space Interpolation

The training with the random sampling forces the latent space towards are Normal distribution.

![](images/paste-23.png){fig-align="center" width="8cm" height="3.4cm"}

Because of that, nearly every point in the latent space has a valid output. This gives us the option of *latent space interpolation*

![](images/paste-24.png){fig-align="center" width="8cm"}

### Hierarchical VAEs

Stacking several VAEs into each other, leads to a deeper encoding of the latent space. This in the end leads to the the network being able to generate images out of noise.

![](images/paste-26.png){fig-align="center" width="9cm"}

### Diffusion Models

A diffusion model is a hierarchical VAE with the following assumptions:

-   All dimensions are the same.
-   All encoder transitions are known Gaussians centred around their previous input.

![](images/paste-28.png){fig-align="center" width="7cm"}

A diffusion model predicts a clean image from a noisy version of the image and is trained like this

![](images/paste-27.png){fig-align="center" width="8cm"}

#### Conditional Diffusion Models

To incorporate conditional information, to control the data generation we give the *Decoder NN* **context**

![](images/paste-29.png){fig-align="center" width="8cm"}

## Natural Language {#sec-naturallanguage}

Natural language is a **sequence of words**. Each word is a discrete unit, predicting the next part of the sequence means predicting words. Same applies for individual letters in a letter-sequence.

### Probabilistic LM-Implementations

#### Tokenization

![](imagesi/paste-99.png){fig-align="center" width="8cm"}

$\textcolor{red}{\text{Attention:}}$ this is easier in english than other languages *(e.g. Chinese).*

#### Vocabularies

A *Vocabulary* is a set of all known words of the model. The **hyperparameter `vocab_size`** defines how many words are in the vocabulary. It is implemented by only keeping the `vocab_size`-most-important words, everything else is replaced by the **UNK** *(unknown)*-Token.

![](images/paste-1.png){fig-align="center" width="8cm" height="3.7cm"}

#### Maths

![](imagesi/paste-100.png){fig-align="center" width="8cm"}

#### Counting

Counting is based on predicting a sentence based on the probability of this sentence appearing in the data. This strategy depends on having instances of sentence prefixes.

![](images/paste-3.png){fig-align="center" width="8cm"}

To mitigate this dependency, **N-gram** counting can be used, that only looks at **N** words at a time.

![](images/paste-4.png){fig-align="center" width="6cm"}

This approach still doesn't understand synonyms for either of the two words: $\textcolor{blue}{\text{He shouted }}\textcolor{orange}{\text{frustratedly}}$ has *probability 0*.

### Attention

With attention we can transfer information from the input to the output. We pass a weighted sum of encoder states to the decoder.

In natural language this can mean, **different words in the output pay attention to different words in the output**

![](imagesi/paste-77.png){fig-align="center" width="3cm" height="3.6cm"}

The "attention" is represented as a weights matrix. It states \*how much attention does output word $j$ pay to input word $i$. Each column shows the importance of the input words to one output words. Each Column **sums to one**.

![](imagesi/paste-78.png){fig-align="center" width="8cm"}

The score of each word (one column) is the **attention alignment score** **function**

$$
\alpha_{t,i} = \text{align}(y_t, x_i) = \frac{\exp(\text{score}(s_{t-1}, h_i))}{\sum_{i'=1}^{n} \exp(\text{score}(s_{t-1}, h_{i'}))}
$$

which is the *softmax of some predefined alignment score*. And this shows how well the two words $y_t$ and $x_i$ are aligned. For the $\text{score}(\dots)$ function there are several options

![](imagesi/paste-79.png){fig-align="center" width="9cm"}

Check [here](https://lilianweng.github.io/posts/2018-06-24-attention/) for more

### Self-Attention

Self-attention computes the output vector $r_i$ for each word via a weighted average of vectors extracted from each word in the input sentence. Here, self-attention learns that "it" should pay attention to "The animal".

![](imagesi/paste-80.png){fig-align="center" width="8cm"}

1.  We compute a $\textcolor{violet}{\text{query vector}}$ for the word and compare it to a $\textcolor{orange}{\text{key vector}}$ for every other word. This computes a $\textcolor{yellow}{\text{score}}$.
2.  For the output vector $r_1$ we sum all $\textcolor{blue}{\text{value vectors}}$ , weighted by the $\textcolor{yellow}{\text{score}}$ in step 1.

![](imagesi/paste-82.png){width="7cm"}

For the [**example**](https://jalammar.github.io/illustrated-transformer/) ***Machines*** we get the 3 vectors:

![](imagesi/paste-88.png){fig-align="center" width="5cm"}

To get the **self-attention for *Thinking***, we do:

![](imagesi/paste-90.png){fig-align="center" width="5cm"}

1.  **Score**: Dot product of the query vector $q_1$ of "Thinking" with the key vectors $\{k_1,k_2\}$ of each word.
2.  **Scale**: Divide each score by $\sqrt{\text{dimensionality } d_k}$ for more stable gradients.
3.  **Softmax**: Apply softmax to transform scores into attention weights
4.  **Weighting**: Multiply with the value vectors $\{v_1,v_2\}$
5.  **Sum**: Sum the weighted value vectors into the final **self-attention for "Thinking"**

#### Computation

![](imagesi/paste-91.png){fig-align="center" width="10cm" height="4.6cm"}

### Multi-Head Attention

Multi-head Attention is used to improve the performance of regular self-attention. We compute self-attention as before some number of times. Call these “attention heads”. The size of the attention heads are smaller than when just using regular self-attention.

To get one set of attention vectors, we concatenate all the heads and apply a linear layer in order to get Z.

![](imagesi/paste-94.png){fig-align="center" width="8cm"}

Multiple heads allow for each head to learn different relationships between words in the sentence. The network ( $W^O$ ) then learns how to handle this. These multiple attentions can be [visualised with this tool](https://colab.research.google.com/github/tens%20orflow/tensor2tensor/blob/master/tensor2ten%20sor/notebooks/hello_t2t.ipynb)

![](imagesi/paste-95.png){fig-align="center" width="6cm"}

### Word Vectors

Words can be stored as **vectors**, the size of the vector is set by the $\textcolor{blue}{\text{embedding sz}}$ and is a hyperparameter. The $\textcolor{orange}{\text{embedding matrix}}$ is learnable and initialised randomly. Each embedding *can* have a meaning, but doesn't have to (trained with the network).

![](imagesi/paste-84.png){fig-align="center" width="9cm"}

The embedding matrix is then used as a lookup table for all the words. (Example, first word in input sentence with ID=2)

![](imagesi/paste-85.png){fig-align="center" width="8cm"}

#### Semantic Directions

In a trained embedded space, words get ordered by (random) meanings, words with the same meaning are clustered. Therefore related word pairs have same distances

![](images/paste-5.png){fig-align="center" width="8cm" height="3.1cm"}

The **similarity** can be quantified through the cosine similarity

$$
\cos(\Theta)=\frac{A\cdot B}{||A||\space||B||}=\frac{\sum_{i=1}^nA_i B_i}{\sqrt{\sum_{i=1}^nA_i^2}\sqrt{\sum_{i=1}^nB_i^2}}
$$

![](images/paste-6.png){fig-align="center" width="5cm"}

### Transformer

Transformers are based on Encoders and Decoders. The big difference to Autoencoders is, that they, are outputting a different language.

The multiple stack up of Encoders and Decoders is for **better performance**.

![](imagesi/paste-86.png){fig-align="center" width="6cm"}

#### Encoder Block Map

If we input a word sequence $\{x_1,x_2\}$, first the **self attention** is applied to get the value vectors $\{z_1,z_2\}$. Then a Feed Forward Network is applied to each word individually.

![](imagesi/paste-87.png){fig-align="center" width="7cm"}

For performance improvments we add **Residuals** to negate vanishing gradients. Furthermore, we apply **LayerNorm**, which improves convergence time

![](imagesi/transformer_resideual_layer_norm.png){fig-align="center" width="7cm"}

#### Positional Encoding

Instead of passing a Embedding vector to the encoder, we pass a **Embedding with Time Signal** vector. Positional encoding can be learned or defined by a fixed function.

![](imagesi/paste-93.png){fig-align="center" width="8cm"}

#### Decoder Block Map

Following the encoder blocks, there are **Decoder Blocks**, these have are very similar to the encoders, but with a additional layer of a **Encoder-Decoder Attention**.

![](imagesi/paste-96.png){fig-align="center" width="9cm"}

The **Encoder-Decoder Attention** takes the output of the encoder instead of the input words

![](imagesi/paste-97.png){fig-align="center" width="10cm" height="3.9cm"}

### Sequence-to-Sequence

Because rule based translation (word for word) often don't work, algorithms are trained on **parallel corpora**, documents which are available in several languages (e.g. legal documents of multilingual countries).

To go from one language to another, a sentence meaning must be encoded in on language and decoded in another language. This can be done through **Attention** or a **LSTM-Autoencoder** architecture.

![](images/paste-15.png)

**Applications**: Seq-to-Seq can also be used for Summaries, Chatbots, part of Speech Tagging, Speech Recognition, Speech Generation.

### Machine Translation Evaluation

There are two measures to evaluate how good a translation is:

-   **BLEU**: **B**i-**L**ingual **E**valuation **U**nderstudy, looking for a fraction of words generated that are in a given ground-truth sentence. Favors shorter sentences.
-   **ROUGE**: **R**ecall-**O**riented **U**nderstudy for **G**isting **E**valuation, looking for a fraction of words in the correct translated sentence, that are in the generated sentence. Favors longer sentences.

This can be combined to the $F_1$-Score:

$$
F_1=\frac{2}{\frac1{BLEU}+\frac1{ROUGE}}=\frac{2(BLEU\cdot ROUGE)}{BLEU+ROUGE}
$$

::: callout-warning
## $F_1$-Score is Dumb

Because the $F_1$-Score can't understand meaning of sentences. Therefore differently translated sentences can be wrongfully punished.

To tackle this Human Evaluation is used.
:::

## Audio and Video Synthesis

### Time Series Forecasting

**Sequence**: Ordered datapoints *(doesn't have to be timedependent)*

**Time-series** (simple univariate): A single series of observations with temporal ordering

$\textcolor{red}{\text{Traditionally}}$ **autoregression** is used to predict time-series, each output is predicted based on past outputs. For example:

$$
X(t)=b_0+b_1*X(t-1)+b_2*X(t-2)
$$

$\textcolor{green}{\text{Deep Learning}}$ **1D CNN**: convolutional layers where the input variable is a window of previous time series values. Can be extended to multivariate data through concatenating outputs of multiple 1D CNNs

![](imagesi/paste-98.png){fig-align="center" width="8cm"}

A well studied example of time-series data is [Natural Language](#sec-naturallanguage).

### Video Generation

Video Generation is using generative models, that learn the probability distribution $p(x)$. They are trained on video datasets like *Kinetics, BAIR Robot Pushing, UCF101 Human Actions*.

There are several approaches to video diffusion:

#### 3D U-Net diffusion model

The simplest approach would be to add the time dimension as a additional dimension to the tensors. The issue with that is the exponential **memory increase** and the **inflexibility** for different length clips.

![](images/paste-54.png){fig-align="center" width="8cm"}

#### Vision Transfomer (ViT)

To get more flexibility with clip length, use a vision transformer.

![](images/paste-55.png){fig-align="center" width="8cm"}

#### TimeSformer

Decompose each frame into a sequence of patches

![](images/paste-56.png){fig-align="center" width="8cm"}

![](images/paste-57.png){fig-align="center" width="8cm"}

## Irregular Networks

### Neural Architecture Search with Reinforcment Learning

To automate the task of finding optimal network structures, we use **Neural Architecture Search**

-   Specify structure using a configuration string: `[“Filter Width: 5”, “Filter Height: 3”, “Num Filters: 24”]`
-   Use a RNN **controller** to generate string
-   Train this architecture (**child network**) to check performance
-   Use **reinforcement learning** to update the parameters of the controller model

![](images/paste-34.png){fig-align="center" width="7cm"}

The controller RNN has this structure:

![](images/paste-35.png){fig-align="center" width="8cm"}

#### Neural Optimizer Search

Optimizers are also hard to design, many different optimizers exist such as ADAM, RMSProp, ADADelta, Momentum, SGD, ...

We can use the previous method to also search over optimizers.

![](images/paste-36.png){fig-align="center" width="8cm"}

![](images/paste-37.png){fig-align="center" width="9cm" height="4.1cm"}

### Graph Convolutional Networks

Graph Convolutional Networks work on **unstructured data**.

**Applications**:

-   Molecular Chemistry: Predict molecular properties
-   Social Network Analysis: Recommender systems for Ads, Events, Products, ...
-   Commonsense Reasoning for AI: Reasokning through access to a database of common facts, stored as a graph

![Commonreasoning end-to-end](images/paste-50.png){fig-align="center" width="8cm"}

**Structure**: A Graph is used to **summarise** data, but there is **no start/end** point $\rightarrow$ cycles.

Architectures for summarising sequences and trees do:

1.  Pass information along relationships between individual elements

![](images/paste-51.png){fig-align="center" width="8cm"}

2.  Aggregating per-element information into a global description of the structure

![](images/paste-52.png){fig-align="center" width="8cm" height="3.7cm"}

![](images/paste-53.png){fig-align="center" width="8cm"}

**Attention**: Just as with RvNNs, batching for MPNNs is very difficult, Cannot pack multiple graphs with different structure into the same big batch tensor. *(use librarys: [Deep Graph Library](https://www.dgl.ai/), [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/))*

## (Energy) Optimized Deep Learning

Deep Learning is extremely power hungry

![](images/paste-38.png){fig-align="center" width="8cm"}

To mitigate this issue, we have to make models more efficient.

**Mid Term**:

-   Prioritizing computationally efficient networks (e.g. [Mobile Net](https://arxiv.org/abs/1704.04861))
-   Report training time, hardware, and *hyperparameter sensitivity* (for awareness)
-   Network Pruning
    1.  Random weight initialization on large network
    2.  After training cut connections with weights $\approx 0$ (only 10-20% have meaningful weights)
    3.  Drop (**prune**) 80-90% of connections and re-train from here on
    4.  $\rightarrow$ increased network speed / decreased power consumption
    5.  **the network trains well, sometimes even *better* than the full network**
-   Network Quantization
    -   Reduce the number of bits representing a number (float32 $\rightarrow$ bfloat16)
    -   Quantization-aware training to reduce precision to four bits with accuracy losses 2-10%
-   Tensor Processing Unit
    -   Special purpose GPU without the graphics part and high throughput at low precision
    -   Specially designed for specific frameworks (Google TPU $\leftarrow$ TensorFlow)
    -   Edge versions for inference only
    -   **TPUs and AI-specialised GPUs are getting more similar**

**Long Term**:

-   More efficient *physical substrates*
    -   Massive GPU parallelism is powerhungry
    -   Are there other physical computing devices? (e.g. **optical compute**)
    -   Neumorphic Computation
-   "Physical" Networks
    -   ASIC, or Co-Processor
    -   Analog Circuits, **memristors**
    -   $\rightarrow$ 1000s of times more energy efficient